<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="color-scheme" content="dark">
    <meta name="theme-color" content="#121212">
    <title>Signal vs. Noise - Daniel Clouse</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <header>
        <h1>Signal vs. Noise</h1>
        <div class="author-info">
            <img src="https://avatars.githubusercontent.com/u/4481780?s=96&v=4" alt="Dan's avatar" class="author-avatar">
            <p>Written by <a href="http://github.com/danielclouse">Dan</a> on July 15, 2025</p>
        </div>
    </header>
    
    <div class="page-container">
        <main>
            <section>
                <p>
                    AI has become a hot topic of discussion lately in many circles of life. In my circle of friends I am one of the few software developers, so some have started asking me about the impacts AI is having on my life, the tech world, and even want to know my predictions about what will happen. One friend of mine who is trying to pivot into programming and software development lamented how AI has changed the job market, and others who work more in analog spaces are puzzled how or why AI would make any difference at all.
                </p>
            <p>
                I want to unpack some of it with less of an eye on AI itself and focus more on people. There's a lot of writing on AI, both as a field and as a product - too much to cover here without losing the thread of my thought. But I want to reinforce the idea that AI is two things: the actual technology, and the product that is being marketed. I'll come back to the technology later, but just the marketing and the story about AI is having a profound impact on its own.
            </p>
            <p>
                The notion of AI for many is as a "smart assistant." In this model, the AI "agent" can take instructions and execute tasks on your behalf, leaving you free to do more complicated or human-focused work. In general, AI is proposed as a tool for generation of images, music, text, and software code nearly instantaneously. There are of course horror stories of AI blackmailing people who threaten to remove it, selling cars for a dollar, and a host of other problematic behaviors. These are incidental problems, but then there are intential uses of AI that create problems, such as websites that allow a person to "nudify" an existing photo of a person- in other words make pornography from the image of an unknowing victim. In other words, it's all very complicated.
            </p>
            <p>
                One thing is clear from the marketing though, is that AI is coming, and it's going to be in absolutely everything. I tried to pay my cellphone bill with my phone this week, and rather than letting me navigate the phone tree with a number pad as I have done for north of a decade, the recorded message proudly boasted the use of AI to help me navigate. Of course it took longer and was more frustrating to use, but it would seem that AI is here to stay, even if it makes things worse.
            </p>
            <p>
                The "agent" model of AI is only part (and a less interesting part if you ask me) of the technology. A truly powerful and compelling use of AI is pattern recognition. For instance, identifying cancer cells or identifying disease outbreaks: this use of the technology gives us insights and understandings of data we already have, making the data more valuable.
            </p>
            <p>
                So in layman's terms, there's AI that can digest and help us understand the data we have, and an AI that can create more data, more content. And this is where I want to veer into talking about people. Humans beings have an innate ability to intuit patterns from the world around us. Moreover, we can then use those patterns to manipulate our world. Language, architecture, art, commerce, cuisine, and culture in general are an expression of that pattern mastery. It's an innate ability, one that gets us into trouble sometimes. For instance, even very bright people are prone to conspiracy theories; that is to say, the information that we all get has enough noise in it that <em>looks like</em> a pattern, a story, an alternative truth, that some people see it as <em>the</em> truth, <em>the</em> story.
            </p>
        </section>

        <section>
            <h2>Noise</h2>
            <p>
                At this point, an estimated $1 Trillion has been invested in AI. With money like that, you can only assume there is now a great deal of pressure to see a return on investment. While AI can be incredibly valuable in specialty markets (like identifying cancer), pulling a profit back out in a short time frame can only be accomplished by selling AI to everyone. That is to say, from what I'm seeing, we're being sold low-value "general" AI because it has the best hope of return, not because it's actually going to be any good for us.
            </p>
            <p>
                The AI hype-train says that AI is going to replace programmers, technical people, white-collar workers, and administrative professionals. Whether AI can actually do the job of those people with anything like competence remains to be seen - I am skeptical on this point - but we're already seeing the side-effects of AI in hiring for the tech sector. I am seeing salaries fall, the number of jobs posted fall, the number of applicants for any given job rise. That friend of mine who is trying to pivot into software development just a couple years ago would have done well. Her intelligence, drive, and skills could propel her into a junior developer position in short order, where she would rapidly grow and mature into a valuable employee. But now there are no junior developer positions posted. The assumption is that senior developers can do that work with AI.
            </p>
            <p>
                As a programmer, I use AI on a daily basis. It makes me into an intensely productive person, in ways that would have been startling even a couple years ago. But I also find myself being very specific in my instructions to the AI agent. It's the kind of specificity that comes with experience, and a lot of it. If I didn't have that experience I might still be able to get things to work, but I would risk putting private or sensitive information out on github, installing obsolete versions of packages, or not using SSL/https. And worse, I would never gain the nuts-and-bolts knowledge of how to get things working. So I could (potentially) deliver working software without getting any better at the craft of delivering working software.
            </p>
            <p>
                So the technology can't replace an experienced professional, but it can prevent the development of future experienced professionals. I suspect that this is the point that represents the greatest risk to organizations. Not that things no longer run, but that no one knows how anything runs anymore, leaving the organization unable to change.
            </p>
            <p>
                There's a techno-futurist view of the problem: the tools will mature fast enough that eventually no humans will be needed. I don't buy this argument for a number of reasons. First is that writing software is one of the more technical and difficult tasks - certainly more difficult than say, driving a car. But getting a computer to drive a car has proven to be intensely difficult, or nigh impossible. There are working examples, but they are either in controlled environments, or carry disclaimers saying they are not for general use. Second, humans are the motivators for change. Over time our interests, objective, and opinions can change, or we can be struck by an intuition or a question. Innovation happens because of change, and it's people that change, not algorithms. AI can act when prompted, but it doesn't act out of initiative. It can't tell a good idea from a bad one, or hold any moral position. It doesn't have any motivators or sense of responsibility. Finally, AI doesn't learn. It can pick up context over a series of prompts and tasks, but it doesn't gain experience or wisdom.
            </p>
        </section>
        
        <section>
            <h2>Signal</h2>
            <p>
                My position is that experience and agency are more valuable than availability and immediacy. As more and more AI-generated content (code, articles, books, music, art) fills the web, less and less of it will represent new or valuable insights. More and more of it will conform to the statistical mean: the words or pixels flow like the amalgamation of all other words or pixels. It's subjugation by mediation.* It's fast, but it's just noise.
            </p>
            <p>
                I am reminded of David Heinemeier Hansson, founder of 37signals, Basecamp, and Hey, announcing that his companies would be leaving the cloud and going back to hosting on their own servers in their own data centers. This involved hiring people with expertise to build and manage the physical infrastructure, as well as the cost of the physical stuff, but netted approximately $1.5 million in savings per year. For a mid-sized company, that's a number that is hard to ignore. The cloud promised reduced cost, complexity, and lock-in but delivered the opposite. I think AI, at least in its current packaging, is headed the same way.
            </p>
            <p>
                Just like the cloud, I think AI can be the right fit for the right problem. But throwing it at everything is going to create a lot of corporate fragility. And throw it at everything they will: by some observations, if every single person who uses ChatGPT is converted to a paying customer, the company would still not be profitable. That means the bill will go up, and the saturation must expand. Or the product and the market collapses.
            </p>
            <p>
                That's a grim prediction, and I'm hoping there is some middle ground between complete elimination of entire fields of work, or collapse of the AI ecosystem. I suspect it will be like the grim predictions printer companies faced at the dawn of email. It was declared that no one would need to print anything ever again because it could be sent electronically. Instead what happened was that printing exploded because there was so much more documentation and communication going back and forth. I suspect that AI is going to land in a similar spot: we'll end up with even more people working to understand and make sense of the noise coming out of the machine and coax it into signal.
            </p>
            <p class="footnote">
                * I started writing this article in an AI-enabled text editor, and had to switch to non-AI tools to allow space for my own thoughts, slow as they were. Otherwise, AI jumped in with a suggestion that <em>seemed</em> like a reasonable end to my sentence, but didn't go where I wanted to go at all.
            </p>
        </section>

        <div style="margin-top: 2rem;">
            <a href="index.html">← Back to Home</a>
        </div>
        </main>
        
        <aside class="right-nav">
            <h2>Explore More</h2>
            <ul>
                <li>
                    <a href="index.html">Home</a>
                    <p>Return to the main page.</p>
                </li>
            </ul>
        </aside>
    </div>

    <footer>
        <p>&copy; <script>document.write(new Date().getFullYear())</script> Daniel Clouse | <a href="https://github.com/danielclouse/danielclouse.github.io">GitHub</a></p>
    </footer>
</body>
</html>
